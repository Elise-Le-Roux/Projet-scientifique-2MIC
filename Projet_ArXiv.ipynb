{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"./arxiv_articles.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous n'utilisons pas le corpus entier mais un sous-ensemble de celui-ci car il est trop long (41 062 documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_doc = 10000 # nombre de documents du corpus que l'on veut utiliser\n",
    "corpus = corpus.loc[0:nb_doc, \"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Calcul du TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions pour le calcul du TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naif_regex_tokenize(text):\n",
    "    \"\"\"\n",
    "    This is a very naif way of tokenize a text. Just using the\n",
    "    regular expression \"[a-z]\" that will match any single word\n",
    "    in lowercase.\n",
    "    Returns a list with all the tokens.\n",
    "    \"\"\"\n",
    "    p = re.compile(\"[a-z]+\")\n",
    "    return p.findall(text.lower())\n",
    "\n",
    "def compute_tf(d):\n",
    "    \"\"\"\n",
    "    Compute the tf for a given document d.\n",
    "    The formula used is \n",
    "    \n",
    "        tf(t, d) = 0.5 + 0.5 * (count(t, d)/max(count(t',d) for t' in d))\n",
    "    \n",
    "    This prevents bias in longer documents.\n",
    "    \"\"\"\n",
    "    terms = pd.Series(naif_regex_tokenize(d))\n",
    "    term_counts = terms.value_counts()\n",
    "    max_tc = max(term_counts)\n",
    "    return 0.5 + 0.5 * (term_counts / max_tc)\n",
    "\n",
    "def compute_idf(D):\n",
    "    \"\"\"\n",
    "    The input D is a list of pandas.Series\n",
    "    having as each element, the term frequency \n",
    "    computed by the function compute_tf.\n",
    "    \"\"\"\n",
    "    N = len(D)\n",
    "    all_terms = pd.concat(D)\n",
    "    nt = all_terms.index.value_counts() # The number of documents containing the term \"t\"\n",
    "    return np.log(N / nt)\n",
    "\n",
    "def compute_tf_idf_document(tf_document, idf):\n",
    "    \"\"\"Compute the tf-idf for each term in a document of the corpus\n",
    "\n",
    "    Keyword arguments:\n",
    "    tf_document -- list with the frequency of each term inside the document\n",
    "    idf -- the idf value for each term in the corpus\n",
    "    \"\"\"\n",
    "    return tf_document * np.array([idf[i] for i in tf_document.index])\n",
    "    \n",
    "def compute_tf_idf_corpus(D):\n",
    "    \"\"\"Compute the tf-idf for each term in a corpus\n",
    "\n",
    "    Keyword arguments:\n",
    "    D -- pandas Series containing a collection of documents in text format\n",
    "    \n",
    "    returns\n",
    "        list of pandas Series containing the tf-idf(t, d, D) for each term\n",
    "        inside each document of the corpus D\n",
    "    \"\"\"\n",
    "    term_freq = [compute_tf(d) for d in D]\n",
    "    idf = compute_idf(term_freq)\n",
    "    return [compute_tf_idf_document(d, idf) for d in term_freq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut obtenir la matrice des TF-IDF de chaque mot dans chaque document :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = compute_tf_idf_corpus(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis la liste des TF-IDF de chaque mot :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_terms = pd.concat(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation d'un dictionnaire de \"stop words\" (mots vides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons un dictionnaire de \"stop words\" afin de retirer les mots tels que \"and\", \"the\" ou encore \"him\" qui ne sont pas informatifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stopwords.words('english'):\n",
    "    if i in all_terms:\n",
    "        all_terms.drop(index=[i], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moyenne des TF-IDF de chaque mot et classement des mots par TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un mot peut apparaître dans plusieurs documents. Il peut donc être associer à plusieurs valeurs de TF-IDF en fonction des documents. Pour pouvoir classer les mots en fonction de leur TF-IDF on fait donc tout d'abord la moyenne de tout les TF-IDF pour chaque mot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tf_idf = all_terms.groupby(all_terms.index).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant classer les mots en fonction de la moyenne de leurs TF-IDF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tf_idf = mean_tf_idf.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Dictionnaire des mots les plus informatifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On considère par exemple qu'un mot qui est présent dans plus de 10 documents différents et qui a une forte valeur de TF-IDF est informatif. La fonction suivante permet de générer un dictionnaire de mots informatifs à partir d'un corpus D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionnaire(D, lenght_dico, K):\n",
    "    dico=[]\n",
    "    \n",
    "    terms = []\n",
    "    for d in D:\n",
    "        terms += list(set(naif_regex_tokenize(d)))\n",
    "    nt = pd.Series(terms).value_counts() # Le nombre de documents contenant le terme \"t\"\n",
    "    \n",
    "    for i in sorted_tf_idf.index:                           # On ajoute dans le dictionnaire les mots apparaissant dans plus\n",
    "        if nt[i]>K and len(i)>1 and len(dico)<lenght_dico:  # de K documents et ayant le plus grand TF_IDF jusqu'à ce que le\n",
    "            dico.append(i)                                  # dictionnaire soit plein ou qu'il n'y ai plus de mots\n",
    "               \n",
    "    return dico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'exemple suivant on génère un dictionnaire contenant les mots présent dans plus de 10 documents et ayant une forte valeur de TF-IDF. La taille du dictionnaire est inférieure ou égale à 100 mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['logic', 'web', 'pulsars', 'polarization', 'agn', 'neutron', 'disc', 'graph', 'international', 'package', 'risk', 'programs', 'pulsar', 'demand', 'rm', 'digital', 'bounds', 'analytic', 'life', 'content', 'wind', 'memory', 'theorem', 'bursts', 'unit', 'structural', 'notion', 'double', 'property', 'extension', 'mu', 'metallicity', 'length', 'generalized', 'stage', 'cherenkov', 'timing', 'detectors', 'architecture', 'overview', 'agent', 'convergence', 'polynomial', 'price', 'weighted', 'utility', 'rule', 'redshift', 'log', 'uniform', 'solving', 'air', 'constructed', 'aspects', 'define', 'companion', 'training', 'semi', 'candidates', 'minimum', 'researchers', 'product', 'bands', 'dense', 'characterization', 'published', 'waves', 'map', 'analytical', 'orders', 'distances', 'upon', 'conditional', 'integrated', 'measures', 'tasks', 'represent', 'maps', 'game', 'varying', 'five', 'modelling', 'date', 'pressure', 'stability', 'brightness', 'agents', 'abundance', 'calculations', 'detail', 'astrophysics', 'works', 'exist', 'usually', 'exoplanet', 'parallel', 'relationship', 'proof', 'run', 'energies', 'programming', 'calculate', 'online', 'calibration', 'history', 'post', 'mode', 'describes', 'questions', 'solve', 'periods', 'popular', 'hard', 'challenging', 'square', 'capable', 'improvement', 'contains', 'clear', 'classes', 'condition', 'gives', 'efficiently', 'variability', 'pre', 'included', 'building', 'modern', 'curve', 'bias', 'equivalent', 'place', 'white', 'volume', 'applying', 'outer', 'description', 'challenge', 'prediction', 'sigma', 'transition', 'ones', 'equation', 'neural', 'enough', 'arbitrary', 'reveal', 'confidence', 'profiles', 'dataset', 'nearly', 'production', 'considering', 'gaussian', 'almost', 'normal', 'argue', 'attention', 'entire', 'commonly', 'orbits', 'every', 'absorption', 'vector', 'technology', 'material', 'classification', 'variation', 'discovered', 'question', 'methodology', 'confirm', 'access', 'public', 'datasets', 'discrete', 'capabilities', 'comparing', 'human', 'showing', 'language', 'statistics', 'angle', 'basis', 'speed', 'estimators', 'construction', 'according', 'response', 'comparable', 'differential', 'promising', 'law', 'depending', 'another', 'index', 'special', 'requirements', 'practice', 'transfer', 'dwarfs', 'synthetic', 'extreme', 'ultra', 'selected', 'states', 'computer', 'final', 'flow', 'molecular', 'groups', 'continuum', 'sufficient', 'body', 'prior', 'samples', 'hot', 'bound', 'estimating', 'reduced', 'established', 'km', 'patterns', 'users', 'reduction', 'elements', 'likelihood', 'version', 'ratios', 'realistic', 'fitting', 'possibly', 'assumption', 'simultaneously', 'original', 'rotation', 'latter', 'beyond', 'seen', 'nuclei', 'feature', 'reported', 'computation', 'alpha', 'year', 'enable', 'cosmological', 'representation', 'scenario', 'located', 'ability', 'chemical', 'wavelengths', 'taking', 'crucial', 'hence', 'populations', 'au', 'formed', 'stable', 'issues', 'composition', 'photometry', 'despite', 'treatment', 'implement', 'generate', 'optimization', 'observe', 'true', 'co', 'cloud', 'points', 'enables', 'difficult', 'last', 'atmosphere', 'adaptive', 'take', 'activity', 'matrix', 'differences', 'binaries', 'positive', 'milky', 'variations', 'challenges', 'scheme', 'environments', 'signals', 'age', 'consists', 'traditional', 'correct', 'practical', 'induced', 'loss', 'shape', 'identification', 'leading', 'center', 'observing', 'uncertainty', 'evaluation', 'zero', 'bright', 'direction', 'predict', 'candidate', 'understand', 'extensive', 'orbit', 'precise', 'forming', 'strategies', 'extremely', 'holes', 'aims', 'specifically', 'already', 'wave', 'decision', 'event', 'broad', 'typically', 'curves', 'uncertainties', 'testing', 'bayesian', 'fact', 'stochastic', 'examine', 'detector', 'sequence', 'position', 'achieved', 'basic', 'possibility', 'allowing', 'social', 'easily', 'task', 'community', 'protoplanetary', 'astronomy', 'reduce', 'continuous', 'scenarios', 'target', 'findings', 'profile', 'computing', 'unknown', 'thermal', 'powerful', 'policy', 'approximation', 'purpose', 'widely', 'difference', 'spectroscopy', 'suitable', 'characterize', 'taken', 'agreement', 'amount', 'choice', 'potentially', 'sensitive', 'mechanisms', 'tests', 'user', 'degree', 'assuming', 'rich', 'introduced', 'discovery', 'instruments', 'establish', 'cm', 'constant', 'motion', 'levels', 'influence', 'supernovae', 'generally', 'needed', 'suggests', 'respectively', 'mission', 'change', 'sun', 'following', 'relation', 'factors', 'origin', 'step', 'weak', 'market', 'includes', 'giant', 'project', 'plane', 'conclude', 'exoplanets', 'astronomical', 'strategy', 'indicate', 'distributed', 'overall', 'follow', 'dwarf', 'classical', 'systematic', 'past', 'critical', 'equations', 'evaluate', 'extend', 'concept', 'makes', 'primary', 'instead', 'depends', 'finding', 'equilibrium', 'experiment', 'interstellar', 'described', 'might', 'domain', 'implemented', 'input', 'existence', 'estimator', 'importance', 'image', 'rather', 'compact', 'particularly', 'alternative', 'correlation', 'atmospheric', 'providing', 'necessary', 'improved', 'disks', 'constrain', 'certain', 'increases', 'dynamical', 'contribution', 'physics', 'defined', 'experimental', 'limits', 'address', 'help', 'whose', 'interactions', 'inner', 'must', 'dependence', 'predicted', 'generated', 'fit', 'supernova', 'combination', 'determined', 'fixed', 'underlying', 'combined', 'towards', 'et', 'selection', 'scientific', 'particles', 'implications', 'goal', 'typical', 'smaller', 'observation', 'construct', 'complexity', 'natural', 'strongly', 'become', 'young', 'spectroscopic', 'aim', 'dark', 'relatively', 'compute', 'luminosity', 'cannot', 'measurement', 'simulated', 'orbital', 'processing', 'regression', 'program', 'leads', 'fully', 'setting', 'universe', 'predictions', 'especially', 'growth', 'per', 'next', 'assumptions', 'art', 'useful', 'end', 'together', 'dependent', 'object', 'respect', 'require', 'wavelength', 'structures', 'masses', 'article', 'radiation', 'cost', 'radius', 'solutions', 'examples', 'random', 'detailed', 'sub', 'hole', 'al', 'errors', 'asymptotic', 'tools', 'interaction', 'scales', 'changes', 'lead', 'knowledge', 'means', 'produced', 'lines', 'corresponding', 'matter', 'procedure', 'economic', 'view', 'complete', 'cluster', 'comparison', 'furthermore', 'upper', 'performed', 'active', 'particle', 'currently', 'fraction', 'ii', 'sets', 'contrast', 'photometric', 'variety', 'robust', 'networks', 'global', 'behavior', 'nearby', 'yet', 'illustrate', 'uses', 'host', 'explain', 'unique', 'nature', 'ground', 'world', 'mechanism', 'quality', 'variables', 'fundamental', 'control', 'dynamic', 'making', 'error', 'clusters', 'increasing', 'types', 'characteristics', 'accretion', 'identified', 'dust', 'discussed', 'achieve', 'area', 'machine', 'group', 'directly', 'term', 'required', 'free', 'studied', 'presents', 'temperature', 'self', 'component', 'provided', 'precision', 'generation', 'deep', 'tool', 'dynamics', 'relevant', 'variable', 'instrument', 'band', 'images', 'accurate', 'radial', 'extended', 'components', 'requires', 'four', 'surveys', 'cross', 'review', 'fast', 'spatial', 'observatory', 'distance', 'estimated', 'inference', 'period', 'magnetic', 'earth', 'major', 'rays', 'produce', 'support', 'computational', 'far', 'modeling', 'probability', 'telescopes', 'distributions', 'angular', 'astrophysical', 'efficiency', 'along', 'implementation', 'imaging', 'moreover', 'medium', 'rates', 'interest', 'driven', 'relative', 'designed', 'core', 'focus', 'previously', 'still', 'environment', 'mean', 'events', 'need', 'part', 'detect', 'infrared', 'open', 'analyze', 'maximum', 'increase', 'software', 'early', 'improve', 'disk', 'role', 'include', 'although', 'account', 'central', 'approaches', 'explore', 'considered', 'whether', 'additional', 'sim', 'total', 'either', 'sensitivity', 'resulting', 'prove', 'estimates', 'observational', 'radio', 'direct', 'independent', 'good', 'limited', 'highly', 'short', 'effective', 'noise', 'frequency', 'context', 'solution', 'background', 'black', 'initial', 'fields', 'array', 'carlo', 'monte', 'full', 'limit', 'across', 'science', 'close', 'specific', 'ratio', 'give', 'planetary', 'made', 'called', 'likely', 'evidence', 'phase', 'perform', 'average', 'series', 'cases', 'sky', 'statistical', 'processes', 'flux', 'region', 'report', 'able', 'development', 'complex', 'surface', 'suggest', 'code', 'larger', 'shown', 'example', 'measured', 'less', 'impact', 'shows', 'factor', 'determine', 'gravitational', 'gamma', 'functions', 'best', 'optimal', 'therefore', 'signal', 'individual', 'least', 'obtain', 'compare', 'magnitude', 'search', 'finite', 'identify', 'accuracy', 'derived', 'recently', 'presence', 'allow', 'constraints', 'experiments', 'values', 'physical', 'velocity', 'population', 'related', 'estimation', 'spectrum', 'understanding', 'often', 'spectra', 'make', 'planet', 'apply', 'regions', 'planets', 'common', 'algorithms', 'develop', 'theoretical', 'empirical', 'addition', 'literature', 'massive', 'detected', 'class', 'derive', 'introduce', 'optical', 'among', 'since', 'network', 'much', 'would', 'measure', 'terms', 'lower', 'associated', 'via', 'significantly', 'better', 'learning', 'technique', 'binary', 'numerical', 'gas', 'value', 'parameter', 'design', 'research', 'dimensional', 'galaxies', 'key', 'efficient', 'simulation', 'higher', 'cosmic', 'line', 'local', 'multiple', 'times', 'features', 'novel', 'techniques', 'wide', 'strong', 'years', 'applied', 'multi', 'problems', 'power', 'existing', 'test', 'similar', 'developed', 'second', 'survey', 'finally', 'presented', 'without', 'near', 'measurements', 'theory', 'application', 'spectral', 'real', 'objects', 'previous', 'various', 'solar', 'point', 'galaxy', 'thus', 'applications', 'size', 'estimate', 'effect', 'form', 'describe', 'compared', 'scale', 'light', 'linear', 'us', 'standard', 'investigate', 'could', 'density', 'consider', 'single', 'rate', 'resolution', 'allows', 'expected', 'potential', 'galactic', 'sources', 'like', 'around', 'process', 'level', 'long', 'conditions', 'provides', 'detection', 'framework', 'consistent', 'simple', 'demonstrate', 'significant', 'found', 'main', 'current', 'way', 'state', 'obtained', 'emission', 'source', 'type', 'studies', 'even', 'available', 'effects', 'algorithm', 'general', 'result', 'sample', 'formation', 'recent', 'evolution', 'telescope', 'structure', 'particular', 'three', 'future', 'ray', 'important', 'case', 'given', 'small', 'known', 'information', 'including', 'within', 'performance', 'stellar', 'parameters', 'discuss', 'possible', 'function', 'due', 'simulations', 'range', 'distribution', 'proposed', 'many', 'energy', 'space', 'propose', 'set', 'problem', 'systems', 'several', 'work', 'methods', 'non', 'star', 'field', 'low', 'mass', 'stars', 'properties', 'order', 'observed', 'however', 'approach', 'number', 'may', 'method', 'system', 'provide', 'observations', 'analysis', 'find', 'different', 'use', 'models', 'first', 'well', 'one', 'large', 'used', 'study', 'time', 'high', 'two', 'new', 'model', 'present', 'based', 'show', 'results', 'also', 'paper', 'using']\n"
     ]
    }
   ],
   "source": [
    "dico = dictionnaire(corpus, 1000, 143)\n",
    "print(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise les termes du dictionnaire afin de créer, pour chaque document, un vecteur TF-IDF. Ce vecteur aura les valeurs du TF-IDF de chaqu'un des termes. Par exemple, si la liste de termes est [\"float\", \"genetic\", \"circular\"] et qu'on a 4 document. On doit produire une matrice de 4 lignes et 3 colonnes :\n",
    "```\n",
    "0.1 5.8 9\n",
    "4.7 1.0 3\n",
    "8.0 2.4 6.0\n",
    "0.3 9.1 3.2\n",
    "```   \n",
    "Ici, chaque ligne contient les valeurs de *tf-idf* de [\"float\", \"genetic\", \"circular\"] (dans cet ordre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(D,dico):\n",
    "    matrice = np.zeros((len(D),len(dico)))\n",
    "\n",
    "    tf_idf = compute_tf_idf_corpus(D)\n",
    "    \n",
    "    for d in range(len(D)):\n",
    "        text_to_list = naif_regex_tokenize(D[d])\n",
    "        for t in range(len(dico)):\n",
    "            if dico[t] in text_to_list:\n",
    "                matrice[d][t]=tf_idf[d][dico[t]]\n",
    "                \n",
    "    return matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice = vectorizer(corpus, dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On transforme ensuite cette matrice en matrice creuse afin qu'elle prenne moins de place en mémoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "matrice_csr = sparse.csr_matrix(matrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant normaliser les lignes de cette matrice. Cette étape permet de meilleurs résultats pour l'algorithme des K-means qui va suivre.\n",
    "La norme 2 des vecteurs TF-IDF représentés par chaque ligne doit être 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.06151802]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "matrix_normalized = normalize(matrice_csr, norm='l2', axis=1)\n",
    "matrice_finale = matrix_normalized.toarray()\n",
    "print(matrice_finale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètre\n",
    "k=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(v1, v2):\n",
    "    \"\"\"\n",
    "    Compute the distance between v1 and v2\n",
    "    v1 and v2 are numpy arrays\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((v1-v2)**2))\n",
    "\n",
    "def assign(vectors, centers):\n",
    "    \"\"\"\n",
    "    assign each vector to the closest center.\n",
    "    vectors is a numpy matrix. We want to assign each\n",
    "    row to the closest center.\n",
    "    centers is a numpy matrix. Each row has a center\n",
    "    \n",
    "    returns a list of integers. \n",
    "    One value for each vector indicating the closest center\n",
    "    \"\"\"\n",
    "    groups = np.zeros(vectors.shape[0])\n",
    "    for i in range(len(groups)):\n",
    "        groups[i] = np.argmin(np.apply_along_axis(distance, 1, centers, vectors[i]))\n",
    "    return groups\n",
    "\n",
    "def compute_centers(vectors, groups):\n",
    "    \"\"\"\n",
    "    Compute the centers for each group of \n",
    "    vectors\n",
    "    vectors is a numpy matrix\n",
    "    groups is a list containing the assignments\n",
    "    of the vectors\n",
    "    \"\"\"\n",
    "    new_centers = np.zeros([int(max(groups)) + 1, vectors.shape[1]])\n",
    "    for i in range(int(max(groups)) + 1):\n",
    "        ix = np.where(groups==i)[0]\n",
    "        grp_members = vectors[ix, :]\n",
    "        new_centers[i] = grp_members.mean(0)\n",
    "    return new_centers\n",
    "\n",
    "def choose_first_centers(vectors, k):\n",
    "    \"\"\"\n",
    "    Select the first k centers for the begining of the\n",
    "    k-means algorithm\n",
    "    \"\"\"\n",
    "    ix = np.arange(0, vectors.shape[0])\n",
    "    np.random.shuffle(ix)\n",
    "    return vectors[ix[:k], :]\n",
    "\n",
    "def kmeans(vectors, k, max_iterations = 500):\n",
    "    \"\"\"\n",
    "    Naive implementation of k-means algorithm\n",
    "    \"\"\"\n",
    "    centers_list = []\n",
    "    centers = choose_first_centers(vectors, k)\n",
    "    centers_list.append(centers)\n",
    "    groups = assign(vectors, centers)\n",
    "    new_centers = compute_centers(vectors, groups)\n",
    "    centers_list.append(new_centers)\n",
    "    nb_iter = 0\n",
    "    while (np.sum(np.abs(centers - new_centers)) > 0) or (nb_iter > max_iterations):\n",
    "        centers = np.copy(new_centers)\n",
    "        groups = assign(vectors, centers)\n",
    "        new_centers = compute_centers(vectors, groups)\n",
    "        centers_list.append(new_centers)\n",
    "        nb_iter += 1\n",
    "    return new_centers, centers_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00019013, 0.00169111, 0.00551591, ..., 0.01783915, 0.01074531,\n",
       "        0.02515515],\n",
       "       [0.00220966, 0.00073618, 0.000176  , ..., 0.01874797, 0.02492312,\n",
       "        0.01641485],\n",
       "       [0.        , 0.00137891, 0.01193273, ..., 0.01642428, 0.01287278,\n",
       "        0.01893995],\n",
       "       ...,\n",
       "       [0.01438069, 0.00057641, 0.        , ..., 0.01544192, 0.02891638,\n",
       "        0.01042481],\n",
       "       [0.        , 0.        , 0.        , ..., 0.02184939, 0.03084578,\n",
       "        0.02058827],\n",
       "       [0.01197778, 0.01732259, 0.        , ..., 0.01588779, 0.03312223,\n",
       "        0.0198688 ]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers, centers_list = kmeans(matrice_finale, k)\n",
    "centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tri des documents en fonction du résultat de l'algorithme k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 0    [10, 14, 35, 57, 74, 111, 150, 186, 188, 189, ...\n",
      "cluster 1    [115, 213, 299, 300, 358, 377, 456, 613, 614, ...\n",
      "cluster 2    [892, 1099, 1991, 2261, 5175, 5178, 5180, 5182...\n",
      "cluster 3    [276, 5181, 5190, 5191, 5202, 5217, 5223, 5230...\n",
      "cluster 4    [1688, 5179, 5188, 5205, 5220, 5221, 5274, 529...\n",
      "cluster 5    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15,...\n",
      "cluster 6    [216, 700, 977, 1057, 1359, 1391, 1397, 1564, ...\n",
      "cluster 7    [45, 101, 133, 337, 480, 522, 558, 668, 703, 7...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "Docs=[]                           # Création d'une série pandas contenant le numéro des documents associés à chaque cluster\n",
    "Index=[]\n",
    "for i in range(k):\n",
    "    Index += [\"cluster \"+str(i)]\n",
    "    Docs += [[]]\n",
    "tri=pd.Series(Docs,index=Index)\n",
    "\n",
    "Assign=assign(matrice_finale, centers) # Grâce à la fonction assign qui assigne chaque vecteur au centre le plus proche,\n",
    "for doc in range(len(Assign)):         # on remplit cette série  \n",
    "    for cluster in range(k):\n",
    "        if int(Assign[doc])==cluster:\n",
    "            Docs[cluster]+=[doc]\n",
    "\n",
    "print(tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BarContainer object of 8 artists>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nombre de documents par cluster\n",
    "import matplotlib.pyplot as plt\n",
    "taille=[]\n",
    "for i in tri.index:\n",
    "    taille+=[len(tri[i])]\n",
    "    \n",
    "print(plt.barh(tri.index, taille, color=\"skyblue\", edgecolor=\"black\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : method\n",
      "Cluster 1 : market\n",
      "Cluster 2 : telescope\n",
      "Cluster 3 : ray\n",
      "Cluster 4 : mass\n",
      "Cluster 5 : prove\n",
      "Cluster 6 : estimation\n",
      "Cluster 7 : paper\n"
     ]
    }
   ],
   "source": [
    "# Titre/Catégorie du cluster\n",
    "# Mot le plus représentatif du cluster (Mot avec le plus grand TF-IDF)\n",
    "\n",
    "c = 0\n",
    "for centre in centers:\n",
    "    indices=[i for i,x in enumerate(centre) if x==max(centre)]\n",
    "    print(\"Cluster\",c,\":\", dico[indices[0]])\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
